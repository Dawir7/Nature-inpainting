{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "\n",
    "img_size = 64 # 64 lub 128\n",
    "data_path = f'data/x{img_size}' # original data path that will be split\n",
    "data_folder = 'data' # folder where data will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data between train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: 25318 train: 15190 val: 5064 test: 5064\n",
      "Fire: 25423 train: 15253 val: 5085 test: 5085\n",
      "Lake: 25236 train: 15141 val: 5047 test: 5048\n",
      "Mountain: 24992 train: 14995 val: 4998 test: 4999\n"
     ]
    }
   ],
   "source": [
    "folder_list = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "\n",
    "train_size = 0.6\n",
    "val_size = 0.2\n",
    "test_size = 0.2\n",
    "\n",
    "labeled_files = {}\n",
    "train_val_test_files = {}\n",
    "for folder in folder_list:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    labeled_files[folder] = os.listdir(folder_path)\n",
    "    \n",
    "    train_val_test_files[folder] = {}\n",
    "\n",
    "    train_files, val_test_files = train_test_split(labeled_files[folder], test_size=1-train_size, random_state=42, shuffle=True)\n",
    "    val_files, test_files = train_test_split(val_test_files, test_size=test_size/(test_size+val_size), random_state=42, shuffle=True)\n",
    "\n",
    "    train_val_test_files[folder]['train'] = train_files\n",
    "    train_val_test_files[folder]['val'] = val_files\n",
    "    train_val_test_files[folder]['test'] = test_files\n",
    "\n",
    "    print(folder+\":\", len(labeled_files[folder]), 'train:', len(train_files), 'val:', len(val_files), 'test:', len(test_files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(source_directory: str, destination_directory: str, files:list[str]):\n",
    "    \"\"\"\n",
    "    Copy specified files from a source directory to a destination directory.\n",
    "\n",
    "    This function iterates over a list of filenames and copies each file from the source directory to the destination directory, if the file does not already exist in the destination directory. It uses `shutil.copy2` to perform the copy operation, which preserves the file's metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_directory : str\n",
    "        The path to the source directory from which files will be copied.\n",
    "    destination_directory : str\n",
    "        The path to the destination directory to which files will be copied.\n",
    "    files : list of str\n",
    "        A list of filenames (strings) that specifies which files to copy from the source directory to the destination directory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    files_in_destination = os.listdir(destination_directory)\n",
    "    for file in tqdm(files, desc=\"Copying files\"):\n",
    "        if file not in files_in_destination:\n",
    "            src_file = os.path.join(source_directory, file)\n",
    "            \n",
    "            if os.path.isfile(src_file):\n",
    "                dst_file = os.path.join(destination_directory, file)\n",
    "                shutil.copy2(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying train files from data/x64/City to data/divided_x64/train/City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 15190/15190 [02:23<00:00, 106.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying train files from data/x64/Fire to data/divided_x64/train/Fire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 15253/15253 [02:20<00:00, 108.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying train files from data/x64/Lake to data/divided_x64/train/Lake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 15141/15141 [02:16<00:00, 111.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying train files from data/x64/Mountain to data/divided_x64/train/Mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 14995/14995 [02:14<00:00, 111.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying val files from data/x64/City to data/divided_x64/val/City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 5064/5064 [00:45<00:00, 112.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying val files from data/x64/Fire to data/divided_x64/val/Fire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 5085/5085 [00:45<00:00, 112.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying val files from data/x64/Lake to data/divided_x64/val/Lake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 5047/5047 [00:45<00:00, 110.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying val files from data/x64/Mountain to data/divided_x64/val/Mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 4998/4998 [00:45<00:00, 110.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying test files from data/x64/City to data/divided_x64/test/City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 5064/5064 [00:45<00:00, 111.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying test files from data/x64/Fire to data/divided_x64/test/Fire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 5085/5085 [00:45<00:00, 112.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying test files from data/x64/Lake to data/divided_x64/test/Lake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 5048/5048 [00:44<00:00, 112.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying test files from data/x64/Mountain to data/divided_x64/test/Mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 4999/4999 [00:46<00:00, 107.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    for folder in folder_list:\n",
    "        source_directory = os.path.join(data_path, folder)\n",
    "        aug_path = f'{data_folder}/divided_x{img_size}'\n",
    "        destination_directory = os.path.join(aug_path, f'{split}', folder)\n",
    "        \n",
    "        os.makedirs(destination_directory, exist_ok=True)\n",
    "        print(f'Copying {split} files from {source_directory} to {destination_directory}')\n",
    "        copy_files(source_directory, destination_directory, train_val_test_files[folder][split])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddRandomBlackRectangle:\n",
    "    \"\"\"\n",
    "    A class to add a randomly sized, positioned, and rotated black rectangle to an image.\n",
    "\n",
    "    This transformation is applied to simulate occlusions in images for data augmentation purposes. \n",
    "    The size, position, and rotation angle of the rectangle are randomly determined within specified limits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_size : int\n",
    "        The minimum size of the rectangle's sides. This value is used as the lower bound for random size generation.\n",
    "    max_size : int\n",
    "        The maximum size of the rectangle's sides. This value is used as the upper bound for random size generation.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(self, img):\n",
    "        Applies the transformation to the given image and returns the modified image.\n",
    "        \"\"\"\n",
    "        \n",
    "    def __init__(self, min_size, max_size):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        rect_width = random.randint(self.min_size, self.max_size)\n",
    "        rect_height = random.randint(self.min_size, self.max_size)\n",
    "        x = random.randint(0, width - rect_width)\n",
    "        y = random.randint(0, height - rect_height)\n",
    "        \n",
    "        # Create a new image with the same size as the original image\n",
    "        rect_img = Image.new('RGB', img.size, (0, 0, 0))\n",
    "        \n",
    "        # Create a new mask image with the same size as the original image\n",
    "        mask_img = Image.new('L', img.size, 0)\n",
    "        \n",
    "        # Draw a rectangle on the new image and the mask\n",
    "        draw_img = ImageDraw.Draw(rect_img)\n",
    "        draw_mask = ImageDraw.Draw(mask_img)\n",
    "        draw_img.rectangle([x, y, x + rect_width, y + rect_height], fill=\"black\")\n",
    "        draw_mask.rectangle([x, y, x + rect_width, y + rect_height], fill=255)\n",
    "        \n",
    "        # Rotate the new image and the mask\n",
    "        angle = random.randint(0, 90)\n",
    "        rect_img = rect_img.rotate(angle)\n",
    "        mask_img = mask_img.rotate(angle)\n",
    "        \n",
    "        # Paste the new image onto the original image using the mask\n",
    "        img.paste(rect_img, (0, 0), mask_img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    AddRandomBlackRectangle(img_size//5, img_size//4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)), # normalize the images to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset_path_train = f'{data_folder}/divided_x{img_size}/train'\n",
    "dataset_path_val = f'{data_folder}/divided_x{img_size}/val'\n",
    "dataset_path_test = f'{data_folder}/divided_x{img_size}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 60579 val: 20194 test: 20196\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.ImageFolder(dataset_path_train, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(dataset_path_val, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(dataset_path_test, transform=transform)\n",
    "\n",
    "print('train', len(train_dataset), 'val:', len(val_dataset), 'test:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_images(data_loader, dataset):\n",
    "    \"\"\"\n",
    "    Saves augmented images from a data loader to disk.\n",
    "\n",
    "    This function iterates over a given data loader, converts each image to a PIL Image format, and saves it to a new \n",
    "    location on disk with an 'augmented_x{img_size}' directory in the path. If the target directory does not exist, it \n",
    "    is created. Each image is saved even if a file with the same name already exists, potentially overwriting existing files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_loader : torch.utils.data.DataLoader\n",
    "        The data loader containing images and labels to be saved. The data loader should yield batches of images and \n",
    "        their corresponding labels.\n",
    "    dataset : torch.utils.data.Dataset\n",
    "        The dataset object associated with the data loader. This is used to retrieve the original file paths of the images \n",
    "        for constructing new file paths for the augmented images.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, (images, labels) in tqdm(enumerate(data_loader), desc='Saving images', total=len(data_loader)):\n",
    "        # Convert tensor to PIL Image\n",
    "        # print(i)\n",
    "        image = transforms.ToPILImage()(images[0])\n",
    "        \n",
    "        # Get the path of the original image\n",
    "        original_path = dataset.samples[i][0].split('/')\n",
    "        original_path[1] = f'augmented_x{img_size}'\n",
    "        original_path = '/'.join(original_path)\n",
    "        \n",
    "        # Create a new path for the transformed image\n",
    "        new_path = os.path.join(original_path)\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "        \n",
    "        # Save the image if the file doesn't exist\n",
    "        if not os.path.exists(new_path):\n",
    "            image.save(new_path)\n",
    "        \n",
    "        # Save the image\n",
    "        image.save(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving images: 100%|██████████| 60579/60579 [13:52<00:00, 72.80it/s]\n",
      "Saving images: 100%|██████████| 20194/20194 [05:01<00:00, 66.89it/s]\n",
      "Saving images: 100%|██████████| 20196/20196 [05:09<00:00, 65.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=1)\n",
    "val_dl = DataLoader(val_dataset, batch_size=1)\n",
    "test_dl = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "save_augmented_images(train_dl, train_dataset)\n",
    "save_augmented_images(val_dl, val_dataset)\n",
    "save_augmented_images(test_dl, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save divided datasets to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from val: 0it [00:00, ?it/s]\n",
      "Loading images from val: 100%|██████████| 5064/5064 [00:21<00:00, 233.00it/s]\n",
      "Loading images from val: 100%|██████████| 5085/5085 [00:20<00:00, 251.33it/s]\n",
      "Loading images from val: 100%|██████████| 5047/5047 [00:20<00:00, 247.34it/s]\n",
      "Loading images from val: 100%|██████████| 4998/4998 [00:20<00:00, 243.62it/s]\n",
      "Loading images from train: 0it [00:00, ?it/s]\n",
      "Loading images from train: 100%|██████████| 15190/15190 [01:01<00:00, 248.53it/s]\n",
      "Loading images from train: 100%|██████████| 15253/15253 [01:01<00:00, 248.43it/s]\n",
      "Loading images from train: 100%|██████████| 15141/15141 [00:58<00:00, 257.31it/s]\n",
      "Loading images from train: 100%|██████████| 14995/14995 [01:01<00:00, 243.27it/s]\n",
      "Loading images from test: 0it [00:00, ?it/s]\n",
      "Loading images from test: 100%|██████████| 5064/5064 [00:20<00:00, 249.32it/s]\n",
      "Loading images from test: 100%|██████████| 5085/5085 [00:20<00:00, 249.01it/s]\n",
      "Loading images from test: 100%|██████████| 5048/5048 [00:20<00:00, 247.48it/s]\n",
      "Loading images from test: 100%|██████████| 4999/4999 [00:20<00:00, 246.16it/s]\n",
      "Loading images from val: 0it [00:00, ?it/s]\n",
      "Loading images from val: 100%|██████████| 5064/5064 [00:22<00:00, 224.77it/s]\n",
      "Loading images from val: 100%|██████████| 5085/5085 [00:22<00:00, 230.20it/s]\n",
      "Loading images from val: 100%|██████████| 5047/5047 [00:21<00:00, 236.12it/s]\n",
      "Loading images from val: 100%|██████████| 4998/4998 [00:21<00:00, 234.37it/s]\n",
      "Loading images from train: 0it [00:00, ?it/s]\n",
      "Loading images from train: 100%|██████████| 15190/15190 [01:05<00:00, 231.81it/s]\n",
      "Loading images from train: 100%|██████████| 15253/15253 [01:05<00:00, 233.33it/s]\n",
      "Loading images from train: 100%|██████████| 15141/15141 [01:02<00:00, 242.56it/s]\n",
      "Loading images from train: 100%|██████████| 14995/14995 [01:04<00:00, 232.82it/s]\n",
      "Loading images from test: 0it [00:00, ?it/s]\n",
      "Loading images from test: 100%|██████████| 5064/5064 [00:20<00:00, 244.09it/s]\n",
      "Loading images from test: 100%|██████████| 5085/5085 [00:22<00:00, 229.39it/s]\n",
      "Loading images from test: 100%|██████████| 5048/5048 [00:21<00:00, 230.36it/s]\n",
      "Loading images from test: 100%|██████████| 4999/4999 [00:21<00:00, 237.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_and_serialize_images(images_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load images from a directory and serialize them into a pickle file.\n",
    "\n",
    "    This function walks through the subdirectories 'val', 'train', and 'test' \n",
    "    in the given directory, loads all JPEG images, and serializes them into a \n",
    "    pickle file. The pickle file is saved in the parent directory of the given \n",
    "    directory, with the name in the format \"{dir_name}_{subdir}.pkl\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images_path : str\n",
    "        The path to the directory containing the images. This directory should \n",
    "        contain the subdirectories 'val', 'train', and 'test'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    subdirs = ['val', 'train', 'test']\n",
    "\n",
    "    dir_name = os.path.basename(os.path.normpath(images_path))\n",
    "    parent_dir = os.path.dirname(os.path.normpath(images_path))\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        image_arrays = []\n",
    "        subdir_path = os.path.join(images_path, subdir)\n",
    "\n",
    "        for root, dirs, files in os.walk(subdir_path):\n",
    "            for file in tqdm(files, desc=f'Loading images from {subdir}'):\n",
    "                if file.endswith('.jpg'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    image_arrays.append(np.array(Image.open(file_path)))\n",
    "\n",
    "        # Serialize the list of numpy arrays with pickle\n",
    "        pickle_file_path = f\"{parent_dir}/{dir_name}/{subdir}.pkl\"\n",
    "        with open(pickle_file_path, 'wb') as f:\n",
    "            pickle.dump(image_arrays, f)\n",
    "\n",
    "\n",
    "load_and_serialize_images(f'{data_folder}/augmented_x64')\n",
    "load_and_serialize_images(f'{data_folder}/divided_x64')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
